# -*- coding: utf-8 -*-
"""Copy of Copy of NN assignment 1 (test)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tdaM43t1bQ2ouuIwu0ySyQkGtBG3xEXR

# 1.

## Uploading *File*s
"""

from google.colab import files
uploaded = files.upload()

"""## Importing relevant libraries"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
import numpy as np
import math
import matplotlib.pyplot as plt
import pandas as pd

import seaborn as sns

"""## Declaration of constants"""

NUM_FEATURES = 7
NUM_CLASSES = 1
NUM_NEURONS = 10

LEARNING_RATE = 1e-3
BETA = 1e-3
EPOCHS = 1000
BATCH_SIZE = 8

SEED = 10
np.random.seed(SEED)

import pandas as pd
df = pd.read_csv('admission_predict.csv')
df.shape

"""## Reading in the data"""

def datagen():

  # Reading in the data from csv 
  admit_data = np.genfromtxt('admission_predict.csv', delimiter= ',')
  # Splitting input variables and output
  X_data, Y_data = admit_data[1:,1:8], admit_data[1:,-1]
  Y_data = Y_data.reshape(Y_data.shape[0], 1)
  
  # Dividing the dataset at 70:30 ratio
  X_train,X_test, y_train, y_test = train_test_split(X_data,Y_data, test_size=0.3, random_state = SEED)# Not necessary to scale the output data

  # Normalization
  X_train = (X_train- np.mean(X_train, axis=0))/ np.std(X_train, axis=0)
  X_test = (X_test- np.mean(X_test, axis=0))/ np.std(X_test, axis=0)
  
  return X_train, X_test, y_train, y_test

"""## Building the 3-layer feed forward network"""

def ffn3(x, num_neuron):
  
  weights1 = tf.Variable(tf.truncated_normal([NUM_FEATURES, num_neuron], stddev=1.0 / np.sqrt(NUM_FEATURES), dtype=tf.float32), name='weights1')
  biases1 = tf.Variable(tf.zeros([num_neuron]), dtype=tf.float32, name='biases1')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights1)
  hidden_layer = tf.nn.relu(tf.matmul(x, weights1) + biases1)

  # ============

  weights2 = tf.Variable(tf.truncated_normal([num_neuron, NUM_CLASSES], stddev=1.0 / np.sqrt(num_neuron), dtype=tf.float32), name='weights2')
  biases2 = tf.Variable(tf.zeros([NUM_CLASSES]), dtype=tf.float32, name='biases2')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights2)
  output = tf.matmul(hidden_layer, weights2) + biases2
  
  
  return output

"""## Training of model"""

def training (batch_size, num_neurons, beta):
  
  X_train, X_test, y_train, y_test = datagen()
  
  # Creation of model
  x = tf.placeholder(tf.float32, [None, NUM_FEATURES])
  y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])

  output = ffn3(x, num_neurons)

  
  # L2 Regularization
  regularizer = tf.contrib.layers.l2_regularizer(scale=beta)
  reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
  reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)

  # Creation of Gradient Descent Optimizer
  optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)
  loss = tf.reduce_mean(tf.square(y_ - output))

  # Loss with L2 regularization
  lossL2 = loss + reg_term
  train_op = optimizer.minimize(lossL2)

  # Training the model
  N = len(X_train)
  train_error = []
  test_error  = []


  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(EPOCHS):
      idx = np.arange(N)
      np.random.shuffle(idx)
      X_train, y_train = X_train[idx], y_train[idx]

      for start, end in zip(range(0, N, batch_size), range(batch_size, N, batch_size)):
        train_op.run(feed_dict={x:X_train[start:end], y_ : y_train[start:end]})


      train_error.append(sess.run(lossL2,feed_dict={x:X_train, y_:y_train}))
      test_error.append(sess.run(lossL2,feed_dict={x:X_test, y_:y_test}))

      if i % 500 == 0:
        print('iter: %d, train_error: %g, test_error: %g'%(i, train_error[i], test_error[i]))
        
    M = len(X_test)
    test_idx = np.arange(M)
    sample_idx = np.random.choice(test_idx, 50)
    X_sample, y_sample = X_test[sample_idx], y_test[sample_idx]

    y_pred = output.eval(feed_dict={x: X_sample})

    prediction = y_pred.flatten()
    target = np.asarray(y_sample).reshape(-1)

    print(list(zip(prediction, target)))   
    
    plt.figure(1)
    plt.scatter(target, prediction)
    # zero error line
    max_test = max(max(target), max(prediction))
    plt.plot([0, max_test], [0, max_test], color='r')
    plt.xlabel('Prediction')
    plt.ylabel('Target')
    plt.title('Prediction Target Plot')
  
  return train_error, test_error

"""## Training the model"""

nn3_train_loss, nn3_test_loss = training(BATCH_SIZE, NUM_NEURONS, BETA)
plt.figure(2,figsize=(20,8))
plt.plot(range(EPOCHS), nn3_train_loss,'b',label='train_error')
plt.plot(range(EPOCHS), nn3_test_loss,'r',label='test_error')
plt.xlabel(str(EPOCHS) + ' iterations')
plt.ylabel('Error')
plt.legend()
# plt.axis([0,2000,0,0.4])

plt.show()

"""## 8x8 Correlation Matrix"""

X_train, X_test, y_train, y_test = datagen()

plt.figure(3,figsize=(20,8))

# Combining the 7 input variables and output variables (test dataset) together
data = np.column_stack([(X_train),(y_train)])

df = pd.DataFrame(data, columns = ['GRE Score','TOEFL Score','University Rating','SOP','LOR','CGPA','Research','Chance of Admit'])
sns.heatmap(df.corr(), center= 0, annot= True)

"""## EarlyStopping: try for 10 epochs, if test_loss > 0.9*max_test_loss == stop training"""

EPOCHS = 1000

# Creation of a model
x = tf.placeholder(tf.float32, [None, NUM_FEATURES])
y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])

output = ffn3(x, NUM_NEURONS)

# L2 Regularization
regularizer = tf.contrib.layers.l2_regularizer(scale=BETA)
reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)

# Creation of Gradient Descent Optimizer
optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)
loss = tf.reduce_mean(tf.square(y_ - output))

# Loss with L2 regularization
lossL2 = loss + reg_term
train_op = optimizer.minimize(lossL2)

# Number of epochs without improvement to tolerate
max_stagnation = 50 
best_test_loss, best_test_epoch = None, None


N = len(X_train)


stopping_step = 0
with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  for epoch in range(EPOCHS):
    idx = np.arange(N)
    np.random.shuffle(idx)
    X_train, y_train = X_train[idx], y_train[idx]
    
    for start, end in zip(range(0, N, BATCH_SIZE), range(BATCH_SIZE, N, BATCH_SIZE)):
      train_op.run(feed_dict={x:X_train[start:end], y_ : y_train[start:end]})
      
    test_loss = sess.run(lossL2, feed_dict={x:X_test, y_ :y_test})

    if best_test_loss is None or best_test_loss*0.9 > test_loss:
        stopping_step = 0
        best_test_loss, best_test_epoch = test_loss, epoch
        print("test loss: ", test_loss, "best test loss: ", best_test_loss, "epoch: ", epoch)

        
    else:
        stopping_step+=1
        print("NO IMPROVEMENT: test_loss: ", test_loss, " best_test_loss: ", best_test_loss, " epoch: ", epoch)

    if stopping_step >=max_stagnation:
        print("early stopping is triggered. best_test_loss: ", best_test_loss, " epoch: ", epoch)
        break

"""## Introduction of functions for RFE"""

def datagenRFE(X_datas, Y_datas, index):

  # Reading in the data from csv 
#   admit_data = np.genfromtxt('admission_predict.csv', delimiter= ',')
  # Splitting input variables and output
#   X_data, Y_data = admit_data[1:,1:8], admit_data[1:,-1]
  
  # Removing each individual columns one by one 
  X_datas = np.delete(X_datas,index, axis = 1)
 
  print("X_Data after dropping: ", X_datas, "\n\n")
  print("X_data shape after dropping: ", X_datas.shape, "\n\n")

  Y_datas = Y_datas.reshape(Y_datas.shape[0], 1)

  # Dividing the dataset at 70:30 ratio
  X_train,X_test, y_train, y_test = train_test_split(X_datas,Y_datas, test_size=0.3, random_state = SEED)# Not necessary to scale the output data

  # Normalization
  X_train = (X_train- np.mean(X_train, axis=0))/ np.std(X_train, axis=0)
  X_test = (X_test- np.mean(X_test, axis=0))/ np.std(X_test, axis=0)

  return X_train, X_test, y_train, y_test

def trainingRFE (batch_size, num_neurons, beta, index,X_data, Y_data, num_features):
  
  X_train, X_test, y_train, y_test = datagenRFE(X_data, Y_data, index)

  # Creation of model
  x = tf.placeholder(tf.float32, [None, num_features])
  y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])

  output = ffn3RFE(x, num_neurons, num_features)


  # L2 Regularization
  regularizer = tf.contrib.layers.l2_regularizer(scale=beta)
  reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
  reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)

  # Creation of Gradient Descent Optimizer
  optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)
  loss = tf.reduce_mean(tf.square(y_ - output))

  # Loss with L2 regularization
  lossL2 = loss + reg_term
  train_op = optimizer.minimize(lossL2)

  # Training the model
  N = len(X_train)
#   print("len of X_train: ", N)
  train_error = []
  test_error  = []
  totalLoss = 0


  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(EPOCHS):

      idx = np.arange(N)

      np.random.shuffle(idx)

      X_train, y_train = X_train[idx], y_train[idx]

      for start, end in zip(range(0, N, batch_size), range(batch_size, N, batch_size)):

#         print("test: ", "1")
        train_op.run(feed_dict={x:X_train[start:end],  y_ : y_train[start:end]})
#         print("test: ", "2")
    test_error = sess.run(lossL2,feed_dict={x:X_test, y_:y_test})
    print("test error: ", test_error)



#       train_error.append(sess.run(lossL2,feed_dict={x:X_train, y_:y_train}))
  
# 
#      test_error = sess.run(lossL2,feed_dict={x:X_test, y_:y_test})

      
#       # Last epoch
#       if i == EPOCHS-1: 

#           totalLoss = sess.run(lossL2,feed_dict={x:X_train, y_:y_train})

              

#       if i % 500 == 0:
#         print('iter: %d, train_error: %g, test_error: %g'%(i, train_error[i], test_error[i]))
    


  return train_error, test_error

def ffn3RFE(x, num_neuron, num_features):

  weights1 = tf.Variable(tf.truncated_normal([num_features, num_neuron], stddev=1.0 / np.sqrt(num_features), dtype=tf.float32), name='weights1')

  biases1 = tf.Variable(tf.zeros([num_neuron]), dtype=tf.float32, name='biases1')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights1)
  hidden_layer = tf.nn.relu(tf.matmul(x, weights1) + biases1)

  # ============

  weights2 = tf.Variable(tf.truncated_normal([num_neuron, NUM_CLASSES], stddev=1.0 / np.sqrt(num_neuron), dtype=tf.float32), name='weights2')

  biases2 = tf.Variable(tf.zeros([NUM_CLASSES]), dtype=tf.float32, name='biases2')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights2)
  output = tf.matmul(hidden_layer, weights2) + biases2
  

  print("output shape: ", output.shape)
  return output

"""## Recursive Feature Elimination"""

EPOCHS = 1000
NUM_FEATURES=6


# def recursion(numVariables, max_test_loss):

# Reading in the data from csv
admit_data = np.genfromtxt('admission_predict.csv', delimiter= ',')
X_data, Y_data = admit_data[1:,1:8], admit_data[1:,-1]

graph = 4
min_best_loss = []
variable_best= []
def recursion (num_features, graph, X_data, Y_data, min_best_loss):
  if num_features == 1:
    return min_best_loss

  
  # Keeping track of the best_loss WITHIN each iteration (7 variables de best_loss, 6 variables de best_loss)
  best_loss = []
  print("input variables count: ",X_data.shape[1]-1 , "\n\n")

  for index in range(X_data.shape[1]):
    print("TRYING \n\n: ", X_data, "\n\n")
    
    RFE_train_loss, RFE_test_loss = trainingRFE(BATCH_SIZE, NUM_NEURONS, BETA, index , X_data, Y_data, num_features)
    print("RFE_test_loss: ", RFE_test_loss)

    best_loss.append(RFE_test_loss)
#     plt.figure(graph,figsize=(20,8))
#     plt.plot(range(EPOCHS), RFE_train_loss,'b',label='train_error')
#     plt.xlabel(str(EPOCHS) + ' iterations')
#     plt.ylabel('Error')
#     plt.legend()

 #   plt.show()
  
  # max_best_loss = variables that results in the lowest accuract, hence, must be removed
  print(best_loss)
  max_best_loss = max(best_loss)
 
  max_best_loss_index = best_loss.index(max_best_loss)
  
  # Drop the variables that results in lowest accuracy
  print("max_best_loss_index: ", max_best_loss_index)
  X_data = np.delete(X_data,max_best_loss_index, axis = 1)
  
  min_best_loss = min(best_loss)
  # Keep tracking of the min_best_loss FOR each iteration
  variable_best.append(min_best_loss)
  
  

  print("variable best: ", variable_best)
  print("max_best_loss_index: ", max_best_loss_index )  
  recursion(num_features-1, graph+1, X_data, Y_data, min_best_loss)
    
  
min_best_loss = recursion(NUM_FEATURES, graph, X_data, Y_data, min_best_loss)
# def recursion (num_features, graph, X_data, Y_data, min_best_loss):

"""## Compare the accuracy of the model with all input features, with models using 6 input features and 5 input features selected using RFE. Comment on the observations."""

NUM_FEATURES = 6
NUM_CLASSES = 1
NUM_NEURONS = 10

LEARNING_RATE = 1e-3
BETA = 1e-3
EPOCHS = 10000
BATCH_SIZE = 8

SEED = 10
np.random.seed(SEED)

def ffn3(x,num_neuron , num_features):
  
  weights1 = tf.Variable(tf.truncated_normal([num_features, num_neuron], stddev=1.0 / np.sqrt(NUM_FEATURES), dtype=tf.float32), name='weights1')
  biases1 = tf.Variable(tf.zeros([num_neuron]), dtype=tf.float32, name='biases1')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights1)
  hidden_layer = tf.nn.relu(tf.matmul(x, weights1) + biases1)

  # ============

  weights2 = tf.Variable(tf.truncated_normal([num_neuron, NUM_CLASSES], stddev=1.0 / np.sqrt(num_neuron), dtype=tf.float32), name='weights2')
  biases2 = tf.Variable(tf.zeros([NUM_CLASSES]), dtype=tf.float32, name='biases2')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights2)
  output = tf.matmul(hidden_layer, weights2) + biases2
  
  
  return output
 
  
  
# ===================================================


def datagen_removal(index):

  # Reading in the data from csv 
  admit_data = np.genfromtxt('admission_predict.csv', delimiter= ',')
 
  # Splitting input variables and output
  X_data, Y_data = admit_data[1:,1:8], admit_data[1:,-1]
  
  # Removal of certain columns
  for i in index: 
    X_data = np.delete(X_data,i, axis = 1)
  
  Y_data = Y_data.reshape(Y_data.shape[0], 1)
  
  # Dividing the dataset at 70:30 ratio
  X_train,X_test, y_train, y_test = train_test_split(X_data,Y_data, test_size=0.3, random_state = SEED)# Not necessary to scale the output data

  # Normalization
  X_train = (X_train- np.mean(X_train, axis=0))/ np.std(X_train, axis=0)
  X_test = (X_test- np.mean(X_test, axis=0))/ np.std(X_test, axis=0)
  
  return X_train, X_test, y_train, y_test




def training (batch_size, num_neurons, beta, index, num_features):
  
  X_train, X_test, y_train, y_test = datagen_removal(index)
  
  # Creation of model
  x = tf.placeholder(tf.float32, [None, num_features])
  y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])

  output = ffn3(x, num_neurons, num_features)

  
  # L2 Regularization
  regularizer = tf.contrib.layers.l2_regularizer(scale=beta)
  reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
  reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)

  # Creation of Gradient Descent Optimizer
  optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)
  loss = tf.reduce_mean(tf.square(y_ - output))

  # Loss with L2 regularization
  lossL2 = loss + reg_term
  train_op = optimizer.minimize(lossL2)

  # Training the model
  N = len(X_train)
  train_error = []
  test_error  = []


  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(EPOCHS):
      idx = np.arange(N)
      np.random.shuffle(idx)
      X_train, y_train = X_train[idx], y_train[idx]

      for start, end in zip(range(0, N, batch_size), range(batch_size, N, batch_size)):
        train_op.run(feed_dict={x:X_train[start:end], y_ : y_train[start:end]})


      train_error.append(sess.run(lossL2,feed_dict={x:X_train, y_:y_train}))
      test_error.append(sess.run(lossL2,feed_dict={x:X_test, y_:y_test}))

      if i % 500 == 0:
        print('iter: %d, train_error: %g, test_error: %g'%(i, train_error[i], test_error[i]))
        

  
  return train_error, test_error




print("6 input features \n\n ")

nn3_train_loss_6features, nn3_test_loss_6features = training(BATCH_SIZE, NUM_NEURONS, BETA, [6], 6)
print("5 input features \n\n")
nn3_train_loss_6features, nn3_test_loss_6features = training(BATCH_SIZE, NUM_NEURONS, BETA, [6,5], 5)

"""## Design a four-layer neural network and a five-layer neural network, with the hidden layers having 50 neurons each. Use a learning rate of 10-3for all layers and optimal feature set selected in part (3).Introduce dropouts (with a keep probability of 0.8) to the layers and report the accuracies. Compare the performances of all the networks (with and without dropouts) with each other and with the 3-layer network."""

KEEP_PROB = 0.8
NUM_FEATURES = 6
NUM_CLASSES = 1
NUM_NEURONS = 50

LEARNING_RATE = 1e-3
BETA = 1e-3
EPOCHS = 5000
BATCH_SIZE = 8

SEED = 10
np.random.seed(SEED)

def datagen_6features():

  # Reading in the data from csv 
  admit_data = np.genfromtxt('admission_predict.csv', delimiter= ',')
  # Splitting input variables and output
  X_data, Y_data = admit_data[1:,1:8], admit_data[1:,-1]
  X_data = np.delete(X_data,6, axis = 1)
  print("X_DATA: ", X_data)
  Y_data = Y_data.reshape(Y_data.shape[0], 1)
  
  
  # Dividing the dataset at 70:30 ratio
  X_train,X_test, y_train, y_test = train_test_split(X_data,Y_data, test_size=0.3, random_state = SEED)# Not necessary to scale the output data

  # Normalization
  X_train = (X_train- np.mean(X_train, axis=0))/ np.std(X_train, axis=0)
  X_test = (X_test- np.mean(X_test, axis=0))/ np.std(X_test, axis=0)
  
  return X_train, X_test, y_train, y_test

"""## Four layer neural network"""

def ffn4(x, num_neuron, keep_prob, dropout):
  
  weights1 = tf.Variable(tf.truncated_normal([NUM_FEATURES, num_neuron], stddev=1.0 / np.sqrt(NUM_FEATURES), dtype=tf.float32), name='weights1')
  biases1 = tf.Variable(tf.zeros([num_neuron]), dtype=tf.float32, name='biases1')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights1)
  hidden_layer1 = tf.nn.relu(tf.matmul(x, weights1) + biases1)
  if dropout:
    hidden_layer1 = tf.nn.dropout(hidden_layer1, keep_prob)

  # ============
  
  weights2 = tf.Variable(tf.truncated_normal([num_neuron, num_neuron], stddev=1.0 / np.sqrt(num_neuron), dtype=tf.float32), name='weights2')
  biases2 = tf.Variable(tf.zeros([num_neuron]), dtype=tf.float32, name='biases2')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights2)
  hidden_layer2 = tf.nn.relu(tf.matmul(hidden_layer1, weights2) + biases2)  
  if dropout:
    hidden_layer2 = tf.nn.dropout(hidden_layer2, keep_prob)

  
  # ============
  

  weights3 = tf.Variable(tf.truncated_normal([num_neuron, NUM_CLASSES], stddev=1.0 / np.sqrt(num_neuron), dtype=tf.float32), name='weights3')
  biases3 = tf.Variable(tf.zeros([NUM_CLASSES]), dtype=tf.float32, name='biases3')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights3)
  output = tf.matmul(hidden_layer2, weights3) + biases3
  
  return output

"""## 5-layer neural network"""

def ffn5(x, num_neuron, keep_prob, dropout):
  
  weights1 = tf.Variable(tf.truncated_normal([NUM_FEATURES, num_neuron], stddev=1.0 / np.sqrt(NUM_FEATURES), dtype=tf.float32), name='weights1')
  biases1 = tf.Variable(tf.zeros([num_neuron]), dtype=tf.float32, name='biases1')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights1)
  hidden_layer1 = tf.nn.relu(tf.matmul(x, weights1) + biases1)
  if dropout:
    hidden_layer1 = tf.nn.dropout(hidden_layer1, keep_prob)

  # ============
  
  weights2 = tf.Variable(tf.truncated_normal([num_neuron, num_neuron], stddev=1.0 / np.sqrt(num_neuron), dtype=tf.float32), name='weights2')
  biases2 = tf.Variable(tf.zeros([num_neuron]), dtype=tf.float32, name='biases2')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights2)
  hidden_layer2 = tf.nn.relu(tf.matmul(hidden_layer1, weights2) + biases2)  
  if dropout:
    hidden_layer2 = tf.nn.dropout(hidden_layer2, keep_prob)

  
  # ============
  
  weights3 = tf.Variable(tf.truncated_normal([num_neuron, num_neuron], stddev=1.0 / np.sqrt(num_neuron), dtype=tf.float32), name='weights3')
  biases3 = tf.Variable(tf.zeros([num_neuron]), dtype=tf.float32, name='biases3')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights3)
  hidden_layer3 = tf.nn.relu(tf.matmul(hidden_layer2, weights3) + biases3)  
  if dropout:
    hidden_layer3 = tf.nn.dropout(hidden_layer3, keep_prob)

  
  # ============
  

  weights4 = tf.Variable(tf.truncated_normal([num_neuron, NUM_CLASSES], stddev=1.0 / np.sqrt(num_neuron), dtype=tf.float32), name='weights4')
  biases4 = tf.Variable(tf.zeros([NUM_CLASSES]), dtype=tf.float32, name='biases4')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights4)
  output = tf.matmul(hidden_layer3, weights4) + biases4
  
  
  return output

def training (batch_size, num_neurons, beta, hiddenLayer_num, keep_prob, dropout):
  
  X_train, X_test, y_train, y_test = datagen_6features()
  
  # Creation of model
  x = tf.placeholder(tf.float32, [None, NUM_FEATURES])
  y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])

  if hiddenLayer_num ==4:
    
    output = ffn4(x, num_neurons, keep_prob, dropout)
  elif hiddenLayer_num ==5:
    output=ffn5(x,num_neurons, keep_prob, dropout)

  
  # L2 Regularization
  regularizer = tf.contrib.layers.l2_regularizer(scale=beta)
  reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
  reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)

  # Creation of Gradient Descent Optimizer
  optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)
  loss = tf.reduce_mean(tf.square(y_ - output))

  # Loss with L2 regularization
  lossL2 = loss + reg_term
  train_op = optimizer.minimize(loss)

  # Training the model
  N = len(X_train)
  train_error = []
  test_error  = []


  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(EPOCHS):
      idx = np.arange(N)
      np.random.shuffle(idx)
      X_train, y_train = X_train[idx], y_train[idx]

      for start, end in zip(range(0, N, batch_size), range(batch_size, N, batch_size)):
        train_op.run(feed_dict={x:X_train[start:end], y_ : y_train[start:end]})


      train_error.append(sess.run(lossL2,feed_dict={x:X_train, y_:y_train}))
      test_error.append(sess.run(lossL2,feed_dict={x:X_test, y_:y_test}))

      if i % 500 == 0:
        print('iter: %d, train_error: %g, test_error: %g'%(i, train_error[i], test_error[i]))
        
 
  
  return train_error, test_error

# NUM_FEATURES = 6
# NUM_CLASSES = 1
# NUM_NEURONS = 50

# LEARNING_RATE = 1e-3
# BETA = 1e-3
# EPOCHS = 10000
# BATCH_SIZE = 8
# DROP_OUT = 0.8

# SEED = 10
# np.random.seed(SEED)


# def training (batch_size, num_neurons, beta, hiddenLayer_num, keep_prob, dropout):


nn4_train_loss_dropout, nn4_test_loss_dropout = training(BATCH_SIZE, NUM_NEURONS, BETA, 4, KEEP_PROB, True)
plt.figure(6,figsize=(20,8))
plt.plot(range(EPOCHS), nn4_test_loss_dropout,'b',label='NN4, dropout, train_error')
# plt.plot(range(EPOCHS), nn3_test_loss,'r',label='test_error')
# plt.xlabel(str(EPOCHS) + ' iterations')
# plt.ylabel('Error')
# plt.legend()

# plt.show()


nn4_train_loss_no_dropout, nn4_test_loss_no_dropout = training(BATCH_SIZE, NUM_NEURONS, BETA, 4, KEEP_PROB, False)
#plt.figure(6,figsize=(20,8))
plt.plot(range(EPOCHS), nn4_test_loss_no_dropout,'r',label='NN4, no dropout, train_error')
# plt.plot(range(EPOCHS), nn3_test_loss,'r',label='test_error')
# plt.xlabel(str(EPOCHS) + ' iterations')
# plt.ylabel('Error')
# plt.legend()

# plt.show()


nn5_train_loss_dropout, nn5_test_loss_dropout = training(BATCH_SIZE, NUM_NEURONS, BETA, 5, KEEP_PROB, True)
#plt.figure(6,figsize=(20,8))
plt.plot(range(EPOCHS), nn5_test_loss_dropout,'g',label='NN5, dropout,train_error ')
# plt.plot(range(EPOCHS), nn3_test_loss,'r',label='test_error')
# plt.xlabel(str(EPOCHS) + ' iterations')
# plt.ylabel('Error')
# plt.legend()

# plt.show()


nn5_train_loss_no_dropout, nn5_test_loss_no_dropout = training(BATCH_SIZE, NUM_NEURONS, BETA, 5, KEEP_PROB, False)
#plt.figure(6,figsize=(20,8))
plt.plot(range(EPOCHS), nn5_test_loss_no_dropout,'black',label='NN5, no dropout, train_error')
# plt.plot(range(EPOCHS), nn3_test_loss,'r',label='test_error')
plt.xlabel(str(EPOCHS) + ' iterations')
plt.ylabel('Error')
plt.legend()

plt.show()