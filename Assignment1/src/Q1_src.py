# -*- coding: utf-8 -*-
"""Question1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oko7xYKUSF9RV1Mlv4ILTh99v4eKbhcq
"""

from google.colab import files
uploaded = files.upload()

import time
import math
from itertools import repeat, product
from functools import partial
import multiprocessing as mp
import tensorflow as tf
import numpy as np
import pandas as pd
import pylab as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold

"""### Exploratory Data Analysis"""

df = pd.read_csv('ctg_data_cleaned.csv')
df.head()

df['NSP'].value_counts()/df.shape[0]*100

plt.figure(figsize=(7,7))
labels = ['Normal','Suspect','Pathologic']
sizes = [0.78,0.14,0.08]

patches = plt.pie(sizes, startangle=90, autopct='%1.1f%%')
plt.legend( labels, loc="best")
plt.axis('equal')
plt.tight_layout()
plt.title('Distribution of NSP Class')
plt.show()

dfx = df.drop(columns = ['CLASS','NSP'])
dfy = df[['NSP']]
X_train1,X_test1, y_train1, y_test1 = train_test_split(dfx,dfy, test_size=0.3, random_state=2019)

y_train1['NSP'].value_counts()

NUM_FEATURES = 21
NUM_CLASSES = 3
num_neurons = 10

lr = 0.01
epochs = 4000
batch_size = 32
beta = 1e-6
split_size = 5

"""#### Normalizing Data"""

# scale data
def scale(X, X_min, X_max):
    return (X - X_min)/(X_max-X_min)

"""#### Data Generation"""

def datagen():
    
    #read data into matrix
    train_input = np.genfromtxt('ctg_data_cleaned.csv', delimiter= ',')
    trainX, train_Y = train_input[1:, :21], train_input[1:,-1].astype(int)
    trainX = scale(trainX, np.min(trainX, axis=0), np.max(trainX, axis=0))

    trainY = np.zeros((train_Y.shape[0], NUM_CLASSES))
    trainY[np.arange(train_Y.shape[0]), train_Y-1] = 1 #one hot matrix

    #split test set
    X_train,X_test, y_train, y_test = train_test_split(trainX,trainY, test_size=0.3, random_state=2019)
 
    return X_train,X_test, y_train, y_test

"""#### Building feed-forward network"""

def ffn3(x,num_neurons):
  
  seed = 10
  np.random.seed(seed)
  
  #Parameters
  w1 = tf.Variable(tf.truncated_normal([NUM_FEATURES, num_neurons], stddev=1.0/math.sqrt(float(NUM_FEATURES))), name='w1')
  b1  = tf.Variable(tf.zeros([num_neurons]), name='b1')

  w2 = tf.Variable(tf.truncated_normal([num_neurons, NUM_CLASSES], stddev=1.0/math.sqrt(float(num_neurons))), name='w2')
  b2  = tf.Variable(tf.zeros([NUM_CLASSES]), name='b2')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, w1)
  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, w2)

  u1 = tf.matmul(x, w1) + b1
  z1 = tf.nn.relu(u1)      # Hidden layer activation: ReLU

  logit = tf.matmul(z1, w2) + b2
  return logit

"""#### Training loop"""

def training(batch_size, num_neurons, beta, iterables): 

  print('Batch size: ' + str(batch_size))
  print('No.of Neurons: ' + str(num_neurons))
  print('Beta values: ' + str(beta))
  neuron_ls = {}
  beta_ls = {}
  batch_ls = {}
  
  X_train,X_test, y_train, y_test = datagen()
  no_data = len(X_train)
  
  #model input
  x = tf.placeholder(tf.float32, [None, NUM_FEATURES])
  y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])
  
   
  logit = ffn3(x,num_neurons)
  

  # Entropy loss
  cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logit)
  loss = tf.reduce_mean(cross_entropy)
  correct_prediction = tf.cast(tf.equal(tf.argmax(logit, 1), tf.argmax(y_, 1)), tf.float32)
  accuracy = tf.reduce_mean(correct_prediction)

  # Loss function using L2 Regularization
  regularizer = tf.contrib.layers.l2_regularizer(scale=beta)
  reg_var = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
  reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_var)
  reg_loss = loss + reg_term       # New loss function with regularisation term

  #learning
  optimizer = tf.train.GradientDescentOptimizer(lr)
  train_op = optimizer.minimize(reg_loss)
    
  
  mean_val_acc = []
  fold_val_acc = []
  fold = 1
  kf = KFold(n_splits=split_size)   # 5-fold CV
  for train_idx, val_idx in kf.split(X_train):
    train_x = X_train[train_idx]
    train_y = y_train[train_idx]
    val_x = X_train[val_idx]
    val_y = y_train[val_idx]

    print('Fold: {}'.format(fold))
    fold = fold + 1
    no_data = len(train_x)
    idx = np.arange(no_data)

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        seed = 10
        np.random.seed(seed)
        acc_val = []
        
        for i in range(epochs):
          np.random.shuffle(idx)
          train_x, train_y = train_x[idx], train_y[idx]

          for start, end in zip(range(0, no_data, batch_size), range(batch_size, no_data, batch_size)):     # run batch size
              train_op.run(feed_dict={x: train_x[start:end], y_: train_y[start:end]})      

          acc_val.append(sess.run(accuracy,feed_dict={x:val_x, y_:val_y}))  # val accuracy

          if i % 100 == 0:
            print('iter %d:Accuracy %g'%(i, acc_val[i]))
            
    fold_val_acc.append(acc_val)   
  
  mean_val_acc = np.mean(fold_val_acc, axis = 0)
  
  neuron_ls[num_neurons] = mean_val_acc
  beta_ls[beta] = mean_val_acc 
  batch_ls[batch_size] = mean_val_acc
  
  if iterables == 'num_neurons':
    return neuron_ls
  elif iterables == 'beta':
    return beta_ls
  else:
    return batch_ls

"""#### 3-Layer Neural Network Architecture"""

epochs = 10000

def nn3(batch_size, num_neurons, beta):


  X_train,X_test, y_train, y_test = datagen()
  no_data = len(X_train)
  
  #model input
  x = tf.placeholder(tf.float32, [None, NUM_FEATURES])
  y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])
  
  
  logit = ffn3(x,num_neurons)
  
  # Entropy loss
  cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logit)
  loss = tf.reduce_mean(cross_entropy)
  correct_prediction = tf.cast(tf.equal(tf.argmax(logit, 1), tf.argmax(y_, 1)), tf.float32)
  accuracy = tf.reduce_mean(correct_prediction)

  # Loss function using L2 Regularization
  regularizer = tf.contrib.layers.l2_regularizer(scale=beta)
  reg_var = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
  reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_var)
  reg_loss = loss + reg_term       # New loss function with regularisation term

  #learning
  optimizer = tf.train.GradientDescentOptimizer(lr)
  train_op = optimizer.minimize(reg_loss)
  
  
  acc_train = []
  acc_test = []
  idx = np.arange(no_data)

  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      
      seed = 10
      np.random.seed(seed)

      for i in range(epochs):
        np.random.shuffle(idx)
        X_train, y_train = X_train[idx], y_train[idx]

        for start, end in zip(range(0, no_data, batch_size), range(batch_size, no_data, batch_size)):     # run batch size
            train_op.run(feed_dict={x: X_train[start:end], y_: y_train[start:end]})      

        acc_train.append(sess.run(accuracy,feed_dict={x:X_train, y_:y_train}))
        acc_test.append(sess.run(accuracy,feed_dict={x:X_test, y_:y_test}))

        if i % 100 == 0:
          print('iter %d:Accuracy  train %g  |  test %g'%(i, acc_train[i],acc_test[i]))
     
  return acc_train, acc_test

nn3_train, nn3_test = nn3(batch_size,num_neurons,beta)
plt.figure(1,figsize=(10,8))
plt.plot(range(epochs), nn3_train,'b',label='train_acc')
plt.plot(range(epochs), nn3_test,'r',label='test_acc')
plt.xlabel(str(epochs) + ' iterations')
plt.ylabel('Accuracy')
plt.title('3-Layer Feedforward Neural Network')
plt.legend()
plt.show()

"""### Comparing Batch Size"""

batches = [4,8,16,32,64]
neuron = 10
beta = 1e-6
iterables='batch_size'
args = zip(batches, repeat(neuron), repeat(beta), repeat(iterables))

no_threads = mp.cpu_count()
p = mp.Pool(processes = no_threads)
costs = p.starmap(training,args)
p.close()
p.join()

plt.figure(1,figsize=(15,12))
colormap = ['r','b','y','g','k']
mapper = 0

for obj in costs:
  for keys in obj.keys():
    plt.plot(range(epochs),obj[keys],colormap[mapper],label='Batch_size: {}'.format(keys))
    mapper+=1
    
plt.xlabel('No of iterations')
plt.ylabel('Accuracy')
plt.title('Cross Validation Accuracy for Network with different batch size')
plt.legend()
plt.show()

"""### Time taken for each batch size"""

X_train,X_test, y_train, y_test = datagen()
no_data = len(X_train)

#model input
x = tf.placeholder(tf.float32, [None, NUM_FEATURES])
y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])


logit = ffn3(x,num_neurons)


# Entropy loss
cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logit)
loss = tf.reduce_mean(cross_entropy)
correct_prediction = tf.cast(tf.equal(tf.argmax(logit, 1), tf.argmax(y_, 1)), tf.float32)
accuracy = tf.reduce_mean(correct_prediction)

# Loss function using L2 Regularization
regularizer = tf.contrib.layers.l2_regularizer(scale=beta)
reg_var = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_var)
reg_loss = loss + reg_term       # New loss function with regularisation term

#learning
optimizer = tf.train.GradientDescentOptimizer(lr)
train_op = optimizer.minimize(reg_loss)


mean_val_acc = []
idx = np.arange(no_data)

batches = [4,8,16,32,64]
for size in batches:    # iterate through various batch sizes
  batch_size = size
  print('batch size: '+ str(batch_size)) 


  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())

      seed = 10
      np.random.seed(seed)

      starttime = time.time()     #start time
      
      for i in range(1):
        for start, end in zip(range(0, no_data, batch_size), range(batch_size, no_data, batch_size)):     # run batch size
            train_op.run(feed_dict={x: X_train[start:end], y_: y_train[start:end]})
            

      endtime = time.time()   #end time

  print(endtime-starttime)

## Validation Phase
# Batch_size = 32

epochs = 4000
train_results = []
test_results = []

train_result, test_result = nn3(16, 10, 1e-6)

plt.figure(1,figsize=(10,8))
plt.plot(range(epochs), train_result,'b',label='train_acc')
plt.plot(range(epochs), test_result,'r',label='test_acc')
plt.xlabel(str(epochs) + ' iterations')
plt.ylabel('Accuracy')
plt.title('Batch Size = 16')
plt.legend()
plt.show()

"""### Comparing number of Neurons"""

neurons =[5,10,15,20,25]
batch_size = 32
beta = 1e-6
iterables='num_neurons'
args = zip(repeat(batch_size), neurons,repeat(beta),repeat(iterables))
no_threads = mp.cpu_count()
p1 = mp.Pool(processes = no_threads)
costs = p1.starmap(training,args)
p1.close()
p1.join()

plt.figure(1,figsize=(15,12))
colormap = ['r','b','y','g','k']
mapper = 0

for obj in costs:
  for keys in obj.keys():
    plt.plot(range(epochs),obj[keys],colormap[mapper],label='Num_neurons: {}'.format(keys))
    mapper+=1
    
plt.xlabel('No of iterations')
plt.ylabel('Accuracy') 
plt.title('Cross Validation Accuracy for Network with different number of hidden neurons')
plt.legend()
plt.show()

"""Looking at the plot, we could observe that hidden neurons = 25 will give the best validation accuracy. However, with a greater number of neurons, the model will be very complex, making it prone to overfitting. Hence, we decided to go with a simpler model using 15 neurons with only a slight trade off in its validation accuracy"""

## Validation Phase
# Num neurons = 15

train_results = []
test_results = []

train_result, test_result = nn3(32, 15, 1e-6)

plt.figure(1,figsize=(10,8))
plt.plot(range(epochs), train_result,'b',label='train_acc')
plt.plot(range(epochs), test_result,'r',label='test_acc')
plt.xlabel(str(epochs) + ' iterations')
plt.ylabel('Accuracy')
plt.title('Number of Neurons = 15')
plt.legend()
plt.show()



"""### Comparing different decay parameter values"""

epoch =  4000
num_neurons = 15
batch_size = 32
iterables = 'beta'
betas =[0.,1e-3,1e-6,1e-9,1e-12]
args = zip(repeat(batch_size), repeat(num_neurons),betas,repeat(iterables))
no_threads = mp.cpu_count()
p2 = mp.Pool(processes = no_threads)
results = p2.starmap(training, args)
p2.close()
p2.join()

plt.figure(1,figsize=(15,12))
colormap = ['r','b','y','g','k']
mapper = 0

for obj in results:
  for keys in obj.keys():
    plt.plot(range(epochs),obj[keys],colormap[mapper],label='Decay_parameter: {}'.format(keys))
    mapper+=1
    
plt.xlabel('No of iterations')
plt.ylabel('Accuracy')
plt.title('Cross Validation Accuracy for Network with different number decay paramater value')
plt.legend()
plt.show()

## Validation Phase
# beta = 0

train_results = []
test_results = []

train_result, test_result = nn3(32, 15, 1e-6)

plt.figure(1,figsize=(10,8))
plt.plot(range(epochs), train_result,'b',label='train_acc')
plt.plot(range(epochs), test_result,'r',label='test_acc')
plt.xlabel(str(epochs) + ' iterations')
plt.ylabel('Accuracy')
plt.title('Decay Parameter = 1e-6')
plt.legend()
plt.show()



"""#### 4 Layer Neural Network Architecture"""

epochs = 10000

def ffn4(x,num_neurons):
  
  seed = 10
  np.random.seed(seed)
  
  #Parameters
  w1 = tf.Variable(tf.truncated_normal([NUM_FEATURES, num_neurons], stddev=1.0/math.sqrt(float(NUM_FEATURES))), name='w1')
  b1  = tf.Variable(tf.zeros([num_neurons]), name='b1')

  w2 = tf.Variable(tf.truncated_normal([num_neurons, num_neurons], stddev=1.0/math.sqrt(float(num_neurons))), name='w2')
  b2  = tf.Variable(tf.zeros([num_neurons]), name='b2')
  
  w3 = tf.Variable(tf.truncated_normal([num_neurons, NUM_CLASSES], stddev=1.0/math.sqrt(float(num_neurons))), name='w3')
  b3  = tf.Variable(tf.zeros([NUM_CLASSES]), name='b3')

  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, w1)
  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, w2)
  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, w3)

  u1 = tf.matmul(x, w1) + b1
  z1 = tf.nn.relu(u1)      # Hidden layer 1 activation: ReLU

  u2 = tf.matmul(z1, w2) + b2
  z2 = tf.nn.relu(u2)     # Hidden layer 2 activation: ReLU
  
  logits = tf.matmul(z2,w3) + b3
  
  return logits





def nn4(batch_size, num_neurons, beta):
  
  X_train,X_test, y_train, y_test = datagen()
  no_data = len(X_train)
  
  #model input
  x = tf.placeholder(tf.float32, [None, NUM_FEATURES])
  y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])
  
  
  logit = ffn4(x,num_neurons)
  
  # Entropy loss
  cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logit)
  loss = tf.reduce_mean(cross_entropy)
  correct_prediction = tf.cast(tf.equal(tf.argmax(logit, 1), tf.argmax(y_, 1)), tf.float32)
  accuracy = tf.reduce_mean(correct_prediction)

  # Loss function using L2 Regularization
  regularizer = tf.contrib.layers.l2_regularizer(scale=beta)
  reg_var = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
  reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_var)
  reg_loss = loss + reg_term       # New loss function with regularisation term

  #learning
  optimizer = tf.train.GradientDescentOptimizer(lr)
  train_op = optimizer.minimize(reg_loss)
  
  
  acc_train = []
  acc_test = []
  idx = np.arange(no_data)

  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      
      seed = 10
      np.random.seed(seed)

      for i in range(epochs):
        np.random.shuffle(idx)
        X_train, y_train = X_train[idx], y_train[idx]

        for start, end in zip(range(0, no_data, batch_size), range(batch_size, no_data, batch_size)):     # run batch size
            train_op.run(feed_dict={x: X_train[start:end], y_: y_train[start:end]})      

        acc_train.append(sess.run(accuracy,feed_dict={x:X_train, y_:y_train}))
        acc_test.append(sess.run(accuracy,feed_dict={x:X_test, y_:y_test}))

        if i % 100 == 0:
          print('iter %d:Accuracy  train %g  |  test %g'%(i, acc_train[i],acc_test[i]))
     
  return acc_train, acc_test

batch_size = 32
beta = 1e-6
lr = 0.01
num_neurons=10

nn4_train, nn4_test = nn4(batch_size,num_neurons,beta)
plt.figure(1,figsize=(10,8))
plt.plot(range(epochs), nn4_train,'b',label='train_acc')
plt.plot(range(epochs), nn4_test,'r',label='test_acc')
plt.xlabel(str(epochs) + ' iterations')
plt.ylabel('Accuracy')
plt.title('4-Layer Feedforward Neural Network')
plt.legend()
plt.show()

nn4_train, nn4_test = nn4(32,10,1e-6)
nn3_train, nn3_test = nn3(16,15,1e-6)
plt.figure(1,figsize=(10,8))
plt.plot(range(epochs), nn4_train,'b-',label='4-layer_train_acc')
plt.plot(range(epochs), nn4_test,'r-',label='4-layer_test_acc')
plt.plot(range(epochs), nn3_train,'y',label='3-layer_train_acc')
plt.plot(range(epochs), nn3_test,'g',label='3-layer_test_acc')
plt.xlabel(str(epochs) + ' iterations')
plt.ylabel('Accuracy')
plt.title('4-Layer vs 3 Layer Feedforward Neural Network')
plt.legend()
plt.show()

