# -*- coding: utf-8 -*-
"""Part_A

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WzRkAH6byteoTiv-VXHaRlaPJ8zejLmz
"""

# Import dependencies
import math
import tensorflow as tf
import numpy as np
import pylab as plt
import pickle

# Constants

NUM_CLASSES = 10
IMG_SIZE = 32
NUM_CHANNELS = 3
learning_rate = 0.001
epochs = 500
batch_size = 128
num_neuron = 300

seed = 10
np.random.seed(seed)
tf.set_random_seed(seed)

# Loading Data
from google.colab import drive
drive.mount('/content/drive')

#loading data
def load_data(file):
    with open(file, 'rb') as fo:
        try:
            samples = pickle.load(fo)
        except UnicodeDecodeError:  #python 3.x
            fo.seek(0)
            samples = pickle.load(fo, encoding='latin1')

    data, labels = samples['data'], samples['labels']

    data = np.array(data, dtype=np.float32)
    labels = np.array(labels, dtype=np.int32)

    
    labels_ = np.zeros([labels.shape[0], NUM_CLASSES])
    labels_[np.arange(labels.shape[0]), labels-1] = 1

    return data, labels_

# CNN Architecture

def cnn(images):

    images = tf.reshape(images, [-1,NUM_CHANNELS, IMG_SIZE, IMG_SIZE])
    images = tf.transpose(images,(0,2,3,1))
    
    #Conv 1
    W1 = tf.Variable(tf.truncated_normal([9, 9, NUM_CHANNELS, 50], stddev=1.0/np.sqrt(NUM_CHANNELS*9*9)), name='weights_1')
    B1 = tf.Variable(tf.zeros([50]), name='biases_1')

    conv_1 = tf.nn.relu(tf.nn.conv2d(images, W1, [1, 1, 1, 1], padding='VALID') + B1)
    pool_1 = tf.nn.max_pool(conv_1, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding='VALID', name='pool_1')

    
    # Conv 2
    W2 = tf.Variable(tf.truncated_normal([5,5,50,60], stddev=1.0/np.sqrt(5*5*50)), name='weights_2')
    B2 = tf.Variable(tf.zeros([60]), name='biases_2')
    
    conv_2 = tf.nn.relu(tf.nn.conv2d(pool_1, W2, strides=[1,1,1,1], padding = 'VALID') + B2)
    pool_2 = tf.nn.max_pool(conv_2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')
    
    dim = pool_2.get_shape()[1].value * pool_2.get_shape()[2].value * pool_2.get_shape()[3].value 
    pool_2_flat = tf.reshape(pool_2, [-1, dim])
    
    
    # Fully Connected Layer
    w1 = tf.Variable(tf.truncated_normal([dim, num_neuron], stddev=1.0/np.sqrt(dim)), name='weights_3')
    b1 = tf.Variable(tf.zeros([num_neuron]), name='biases_3')
    u1 = tf.matmul(pool_2_flat,w1) + b1
    y1 = tf.nn.relu(u1)

    #Softmax
    w2 = tf.Variable(tf.truncated_normal([num_neuron, NUM_CLASSES], stddev=1.0/np.sqrt(num_neuron)), name='weights_4')
    b2 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases_4')
    logits = tf.matmul(y1, w2) + b2

    return conv_1, pool_1, conv_2, pool_2, logits

"""#### CNN Training"""

# Main function

def main_1():

    trainX, trainY = load_data('/content/drive/My Drive/CZ4042 Coursework/Ass2/data/data_batch_1')
    print(trainX.shape, trainY.shape)
    
    testX, testY = load_data('/content/drive/My Drive/CZ4042 Coursework/Ass2/data/test_batch_trim')
    print(testX.shape, testY.shape)

    trainX = (trainX - np.min(trainX, axis = 0))/np.max(trainX, axis = 0)

    # Create the model
    x = tf.placeholder(tf.float32, [None, IMG_SIZE*IMG_SIZE*NUM_CHANNELS])
    y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])

    
    conv_1, pool_1, conv_2, pool_2, logits = cnn(x)
    
    # Loss Function
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logits)
    loss = tf.reduce_mean(cross_entropy)
    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)
    
    correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y_,1))
    correct_pred = tf.cast(correct_pred, tf.float32)
    accuracy = tf.reduce_mean(correct_pred)
    
    # Training
    N = len(trainX)
    idx = np.arange(N)
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        
        train_loss = []
        train_acc = []
        test_acc = []
        for e in range(epochs):
            np.random.shuffle(idx)
            trainX, trainY = trainX[idx], trainY[idx]
            
            for start, end in zip(range(0, N, batch_size), range(batch_size, N, batch_size)):
              train_step.run(feed_dict={x: trainX[start:end], y_: trainY[start:end]})

            train_loss.append(loss.eval(feed_dict={x: trainX, y_: trainY}))
            train_acc.append(accuracy.eval(feed_dict={x: trainX, y_: trainY}))
            test_acc.append(accuracy.eval(feed_dict={x: testX, y_: testY,}))

            
            if e%100 == 0:
              print('epoch', e, 'entropy', train_loss[e])
              print(accuracy.eval(feed_dict={x: testX, y_: testY,}))
              
        # Plotting test accuracy
        plt.figure(figsize=(10,8))
        plt.plot(np.arange(epochs), train_loss, label='train_loss')   
        plt.plot(np.arange(epochs), test_acc, label='test_acc')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.title('Test accuracy against epochs')

        # # Plotting train loss
        # plt.figure(figsize=(10,8))
        # plt.plot(np.arange(epochs), train_loss, label='train_loss')   
        # plt.xlabel('Epochs')
        # plt.ylabel('Loss')
        # plt.legend()
        # plt.title('Train loss against epochs')
    
        for i in range(2):
          ind = np.random.randint(low=0, high=10000)
          X = trainX[ind,:]
                
          h_conv1, h_pool1, h_conv2, h_pool2 = sess.run([conv_1, pool_1, conv_2, pool_2], {x: X.reshape(1,IMG_SIZE*IMG_SIZE*NUM_CHANNELS)})
      
          plt.figure()
          plt.title('Input Image {}'.format(x))
          plt.gray()
          X_show = X.reshape(NUM_CHANNELS, IMG_SIZE, IMG_SIZE).transpose(1, 2, 0)
          plt.axis('off')
          plt.imshow(X_show)

          # Conv1 feature map
          plt.figure()
          plt.gray()
          h_conv1 = np.array(h_conv1)
          for i in range(25):
              plt.subplot(5, 5, i+1); plt.axis('off'); plt.imshow(h_conv1[0,:,:,i])
          

          # Pool1 feature maps
          plt.figure()
          plt.gray()
          h_pool1 = np.array(h_pool1)
          for i in range(25):
              plt.subplot(5, 5, i+1); plt.axis('off'); plt.imshow(h_pool1[0,:,:,i])
              
          # Conv2 feature maps    
          plt.figure()
          plt.gray()
          h_conv2 = np.array(h_conv2)
          for i in range(25):
              plt.subplot(5, 5, i+1); plt.axis('off'); plt.imshow(h_conv2[0,:,:,i])
          

          # Pool2 feature maps
          plt.figure()
          plt.gray()
          h_pool2 = np.array(h_pool2)
          for i in range(25):
              plt.subplot(5, 5, i+1); plt.axis('off'); plt.imshow(h_pool2[0,:,:,i])
          
          


if __name__ == "__main__":
  main_1()

"""#### Grid Search"""

# CNN with Grid Search

def cnn_grid(images,layer1,layer2):    #layer1 layer2 represents number of filters used in the conv layer

    print("Conv1 depth:{}           Conv2 depth:{}".format(layer1,layer2))

    images = tf.reshape(images, [-1, NUM_CHANNELS, IMG_SIZE, IMG_SIZE])
    images = tf.transpose(images,(0,2,3,1))
    
    #Conv 1
    W1 = tf.Variable(tf.truncated_normal([9, 9, NUM_CHANNELS, layer1], stddev=1.0/np.sqrt(NUM_CHANNELS*9*9)), name='weights_1')
    B1 = tf.Variable(tf.zeros([layer1]), name='biases_1')

    conv_1 = tf.nn.relu(tf.nn.conv2d(images, W1, [1, 1, 1, 1], padding='VALID') + B1)
    pool_1 = tf.nn.max_pool(conv_1, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding='VALID', name='pool_1')

    
    # Conv 2
    W2 = tf.Variable(tf.truncated_normal([5,5,layer1,layer2], stddev=1.0/np.sqrt(5*5*layer1)), name='weights_2')
    B2 = tf.Variable(tf.zeros([layer2]), name='biases_2')
    
    conv_2 = tf.nn.relu(tf.nn.conv2d(pool_1, W2, strides=[1,1,1,1], padding = 'VALID') + B2)
    pool_2 = tf.nn.max_pool(conv_2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')
    
    dim = pool_2.get_shape()[1].value * pool_2.get_shape()[2].value * pool_2.get_shape()[3].value 
    pool_2_flat = tf.reshape(pool_2, [-1, dim])
    
    
    # Fully Connected Layer
    w1 = tf.Variable(tf.truncated_normal([dim, num_neuron], stddev=1.0/np.sqrt(dim)), name='weights_3')
    b1 = tf.Variable(tf.zeros([num_neuron]), name='biases_3')
    u1 = tf.matmul(pool_2_flat,w1) + b1
    y1 = tf.nn.relu(u1)

    #Softmax
    w2 = tf.Variable(tf.truncated_normal([num_neuron, NUM_CLASSES], stddev=1.0/np.sqrt(num_neuron)), name='weights_4')
    b2 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases_4')
    logits = tf.matmul(y1, w2) + b2

    return conv_1, pool_1, conv_2, pool_2, logits

# Main function

def main_2(layer1,layer2):

    trainX, trainY = load_data('/content/drive/My Drive/CZ4042 Coursework/Ass2/data/data_batch_1')
    #print(trainX.shape, trainY.shape)
    
    testX, testY = load_data('/content/drive/My Drive/CZ4042 Coursework/Ass2/data/test_batch_trim')
    #print(testX.shape, testY.shape)

    # Normalize features 
    trainX = (trainX - np.min(trainX, axis = 0))/np.max(trainX, axis = 0)

    # Create the model
    x = tf.placeholder(tf.float32, [None, IMG_SIZE*IMG_SIZE*NUM_CHANNELS])
    y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])

    conv_1, pool_1, conv_2, pool_2, logits = cnn_grid(x,layer1,layer2)
    
    # Loss Function
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logits)
    loss = tf.reduce_mean(cross_entropy)
    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)
    
    correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y_,1))
    correct_pred = tf.cast(correct_pred, tf.float32)
    accuracy = tf.reduce_mean(correct_pred)
    
    # Training
    epochs = 500          
    N = len(trainX)
    idx = np.arange(N)
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        
        #test_acc = []
        train_loss = []
        for e in range(epochs):
            np.random.shuffle(idx)
            trainX, trainY = trainX[idx], trainY[idx]
            
            for start, end in zip(range(0, N, batch_size), range(batch_size, N, batch_size)):
              train_step.run(feed_dict={x: trainX[start:end], y_: trainY[start:end]})

            train_loss.append(loss.eval(feed_dict={x: trainX, y_: trainY}))
            #test_acc.append(accuracy.eval(feed_dict={x: testX, y_: testY,}))

            
            if e%100 == 0:
              print('epoch', e, 'entropy', train_loss[e])
    
        test_accuracy = accuracy.eval(feed_dict={x: testX, y_: testY,})
        print('Test Accuracy = {}'.format(test_accuracy))
              
        # # Plotting train and test accuracy
        # plt.figure(figsize=(10,8))
        # #plt.plot(np.arange(epochs), train_loss, label='train_loss')   
        # plt.plot(np.arange(epochs), test_acc, label='test_acc')
        # plt.plot(np.arange(epochs), train_acc, label='train_acc')
        # plt.xlabel('Epochs')
        # plt.ylabel('Accuracy')
        # plt.legend()
        # plt.title('CNN with Gradient Descent Optimizer')

# Grid Searching

def grid_search():
  c1 = [10,32,32,64,64,128,128,256,256]
  c2 = [10,32,64,64,128,128,256,256,512]
  for layer1,layer2 in zip(c1,c2):
    main_2(layer1,layer2)

if __name__ == '__main__':
  grid_search()



"""#### Optimizers"""

# CNN with optimizer

def cnn_opt(images,layer1,layer2,dropout,rate=0.7):    

    print("Conv1 depth:{}           Conv2 depth:{}".format(layer1,layer2))

    images = tf.reshape(images, [-1, NUM_CHANNELS, IMG_SIZE, IMG_SIZE])
    images = tf.transpose(images,(0,2,3,1))
    
    #Conv 1
    W1 = tf.Variable(tf.truncated_normal([9, 9, NUM_CHANNELS, layer1], stddev=1.0/np.sqrt(NUM_CHANNELS*9*9)), name='weights_1')
    B1 = tf.Variable(tf.zeros([layer1]), name='biases_1')

    conv_1 = tf.nn.relu(tf.nn.conv2d(images, W1, [1, 1, 1, 1], padding='VALID') + B1)
    pool_1 = tf.nn.max_pool(conv_1, ksize= [1, 2, 2, 1], strides= [1, 2, 2, 1], padding='VALID', name='pool_1')

    
    # Conv 2
    W2 = tf.Variable(tf.truncated_normal([5,5,layer1,layer2], stddev=1.0/np.sqrt(5*5*layer1)), name='weights_2')
    B2 = tf.Variable(tf.zeros([layer2]), name='biases_2')
    
    conv_2 = tf.nn.relu(tf.nn.conv2d(pool_1, W2, strides=[1,1,1,1], padding = 'VALID') + B2)
    pool_2 = tf.nn.max_pool(conv_2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')
    
    dim = pool_2.get_shape()[1].value * pool_2.get_shape()[2].value * pool_2.get_shape()[3].value 
    pool_2_flat = tf.reshape(pool_2, [-1, dim])
    
    
    # Fully Connected Layer
    w1 = tf.Variable(tf.truncated_normal([dim, num_neuron], stddev=1.0/np.sqrt(dim)), name='weights_3')
    b1 = tf.Variable(tf.zeros([num_neuron]), name='biases_3')
    u1 = tf.matmul(pool_2_flat,w1) + b1
    y1 = tf.nn.relu(u1)
    if dropout:
      y1 = tf.nn.dropout(y1, keep_prob = rate)

    #Softmax
    w2 = tf.Variable(tf.truncated_normal([num_neuron, NUM_CLASSES], stddev=1.0/np.sqrt(num_neuron)), name='weights_4')
    b2 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases_4')
    logits = tf.matmul(y1, w2) + b2

    return conv_1, pool_1, conv_2, pool_2, logits

def main_3(layer1,layer2,opt,dropout):    #opt: type of otimizer    dropout: bool

    trainX, trainY = load_data('/content/drive/My Drive/CZ4042 Coursework/Ass2/data/data_batch_1')
    #print(trainX.shape, trainY.shape)
    
    testX, testY = load_data('/content/drive/My Drive/CZ4042 Coursework/Ass2/data/test_batch_trim')
    #print(testX.shape, testY.shape)

    # Normalize features 
    trainX = (trainX - np.min(trainX, axis = 0))/np.max(trainX, axis = 0)

    # Create the model
    x = tf.placeholder(tf.float32, [None, IMG_SIZE*IMG_SIZE*NUM_CHANNELS])
    y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES])

    conv_1, pool_1, conv_2, pool_2, logits = cnn_opt(x,layer1,layer2,dropout)
    
    # Loss Function
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logits)
    loss = tf.reduce_mean(cross_entropy)
    if opt == 'momentum':
      optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.1)
    elif opt == 'rms':
      optimizer = tf.train.RMSPropOptimizer(learning_rate)
    elif opt == 'adam':
      optimizer = tf.train.AdamOptimizer(learning_rate)
    else:
      optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train_step = optimizer.minimize(loss)
    
    correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y_,1))
    correct_pred = tf.cast(correct_pred, tf.float32)
    accuracy = tf.reduce_mean(correct_pred)
    
    # Training
    epochs = 500          
    N = len(trainX)
    idx = np.arange(N)
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        
        #test_acc = []
        train_loss = []
        test_acc = []
        for e in range(epochs):
            np.random.shuffle(idx)
            trainX, trainY = trainX[idx], trainY[idx]
            
            for start, end in zip(range(0, N, batch_size), range(batch_size, N, batch_size)):
              train_step.run(feed_dict={x: trainX[start:end], y_: trainY[start:end]})

            train_loss.append(loss.eval(feed_dict={x: trainX, y_: trainY}))
            test_acc.append(accuracy.eval(feed_dict={x: testX, y_: testY,}))

            if e%100 == 0:
              print('epoch', e, 'entropy', train_loss[e])

        acc = accuracy.eval(feed_dict={x: testX, y_: testY,})
        print(acc)
    
       
              
        # Plotting Test accuracy
        # plt.figure(figsize=(10,8))  
        # plt.plot(np.arange(epochs), test_acc, label='test_acc')
        # plt.xlabel('Epochs')
        # plt.ylabel('Accuracy')
        # plt.title('CNN with {}'.format(opt))

        #Plot Train loss
        plt.figure(figsize=(10,8))
        plt.plot(np.arange(epochs), train_loss, label='train_loss')
        plt.plot(np.arange(epochs), test_acc, label='test_acc')
        plt.legend()   
        plt.xlabel('Epochs')
        plt.ylabel('Train Loss & Test Accuracy')
        plt.title('CNN with {}'.format(opt))

# Momentum Optimizer
main_3(256,512,'momentum',0)

# RMSProp Optimizer
main_3(256,512,'rms',0)

# Adam Optimizer
main_3(256,512,'adam',0)

# SGD with dropout
main_3(256,512,'sgd',1)























"""### Keras"""

from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
from keras.layers import Dropout
from matplotlib import pyplot
from keras.models import Model
from numpy import expand_dims
# evaluate the deep model on the test dataset
from keras.datasets import cifar10
from keras.models import load_model
from keras.utils import to_categorical

def load_dataset():
	# load dataset
	(trainX, trainY), (testX, testY) = cifar10.load_data()
	# one hot encode target values
	trainY = to_categorical(trainY)
	testY = to_categorical(testY)
	return trainX, trainY, testX, testY

def define_model():
	model = Sequential()
	model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer="TruncatedNormal", padding='valid', input_shape=(IMG_SIZE,IMG_SIZE,3)))
	model.add(MaxPooling2D((2, 2), strides = (2,2), padding = 'valid'))
	model.add(Conv2D(60, (5, 5), activation='relu', kernel_initializer="TruncatedNormal", padding='valid'))
	model.add(MaxPooling2D((2, 2), strides = (2,2), padding = 'valid'))
	model.add(Flatten())
	model.add(Dense(300, activation='relu', kernel_initializer="TruncatedNormal"))
	model.add(Dense(10, activation='softmax'))
	# compile model
	opt = SGD(lr=0.001)
	model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
	return model

# define cnn model
def define_model():
	model = Sequential()
	model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
	model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
	model.add(MaxPooling2D((2, 2)))
	model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
	model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
	model.add(MaxPooling2D((2, 2)))
	model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
	model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
	model.add(MaxPooling2D((2, 2)))
	model.add(Flatten())
	model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
	model.add(Dense(10, activation='softmax'))
	# compile model
	opt = SGD(lr=0.001, momentum=0.9)
	model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
	return model

def summarize_diagnostics(history):
  # plot loss
  pyplot.figure(figsize=(10,8))
  pyplot.subplot(211)
  pyplot.title('Cross Entropy Loss')
  pyplot.plot(history.history['loss'], color='blue', label='train')
  pyplot.plot(history.history['val_loss'], color='orange', label='test')
  # plot accuracy
  pyplot.subplot(212)
  pyplot.title('Classification Accuracy')
  pyplot.plot(history.history['acc'], color='blue', label='train')
  pyplot.plot(history.history['val_acc'], color='orange', label='test')

def run_test():
  trainX, trainY, testX, testY = load_dataset()
  model = define_model()
  # fit model
  history = model.fit(trainX, trainY, epochs=100, batch_size=128, validation_data=(testX, testY), verbose=0)
  # evaluate model
  history_dict = history.history
  print(history_dict.keys())
  _, acc = model.evaluate(testX, testY, verbose=0)
  print('> %.3f' % (acc * 100.0))
  # learning curves
  summarize_diagnostics(history)

run_test()

"""### Feature Maps"""

trainX, trainY = load_data('data_batch_1')
  trainX = (trainX - np.min(trainX, axis = 0))/np.max(trainX, axis = 0)
  ind = np.random.randint(low=0, high=10000)
  X = trainX[ind,:]

  plt.figure()
  plt.title('Input Image')
  plt.gray()
  X_show = X.reshape(NUM_CHANNELS, IMG_SIZE, IMG_SIZE).transpose(1, 2, 0)
  plt.axis('off')
  plt.imshow(X_show)

model = define_model()
# summarize feature map shapes
for i in range(len(model.layers)):
	layer = model.layers[i]
	# summarize output shape
	print(i, layer.name, layer.output.shape)

# redefine model to output right after the first hidden layer
conv1 = Model(inputs=model.inputs, outputs=model.layers[0].output)
pool1 = Model(inputs=model.inputs, outputs=model.layers[1].output)
conv2 = Model(inputs=model.inputs, outputs=model.layers[2].output)
pool2 = Model(inputs=model.inputs, outputs=model.layers[3].output)

def feature_map(X, layer):

  X = np.reshape(X, (IMG_SIZE, IMG_SIZE, NUM_CHANNELS))
  img = expand_dims(X, axis=0)
  feature_map = layer.predict(img)
  
  square = 5
  ix = 1
  for _ in range(square):
    for _ in range(square):
      # specify subplot and turn of axis
      ax = pyplot.subplot(square, square, ix)
      ax.set_xticks([])
      ax.set_yticks([])
      # plot filter channel in grayscale
      pyplot.imshow(feature_map[0, :, :, ix-1], cmap='gray')
      ix += 1
  # show the figure
  pyplot.show()

feature_map(X, conv1)

feature_map(X, conv1)

feature_map(X, conv2)

